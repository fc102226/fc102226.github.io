---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I work at TikTok <img src='./images/hh.png' style='width: 6em;'> as a research scientist now in Singapore. 

I am now working on audio-driven talking face generation, text-to-speech and music generation research. If you are seeking any form of **academic cooperation**, please feel free to email me at [ren.yi@bytedance.com](mailto:ren.yi@bytedance.com). We are hiring interns!

I graduated from [Chu Kochen Honors College](http://ckc.zju.edu.cn/ckcen/main.htm), Zhejiang University (ÊµôÊ±üÂ§ßÂ≠¶Á´∫ÂèØÊ°¢Â≠¶Èô¢) with a bachelor's degree and from the Department of Computer Science and Technology, Zhejiang University (ÊµôÊ±üÂ§ßÂ≠¶ËÆ°ÁÆóÊú∫ÁßëÂ≠¶‰∏éÊäÄÊúØÂ≠¶Èô¢) with a master's degree, advised by [Zhou Zhao (ËµµÊ¥≤)](https://person.zju.edu.cn/zhaozhou). I also collaborate with [Xu Tan (Ë∞≠Êó≠)](https://www.microsoft.com/en-us/research/people/xuta/), [Tao Qin (Áß¶Ê∂õ)](https://www.microsoft.com/en-us/research/people/taoqin/) and [Tie-yan Liu (ÂàòÈìÅÂ≤©)](https://www.microsoft.com/en-us/research/people/tyliu/) from [Microsoft Research Asia](https://www.microsoft.com/en-us/research/group/machine-learning-research-group/) <img src='./images/hh.png' style="width: 4em;"> closely. 

I won the [Baidu Scholarship](https://baike.baidu.com/item/%E7%99%BE%E5%BA%A6%E5%A5%96%E5%AD%A6%E9%87%91/9929412) (10 candidates worldwide each year) and [ByteDance Scholars Program](https://ur.bytedance.com/scholarship) (10 candidates worldwide each year) in 2020 and was selected as one of [the top 100 AI Chinese new stars](https://mp.weixin.qq.com/s?__biz=MzA4NzQ5MTA2NA==&mid=2653639431&idx=1&sn=25b6368c1954419b9090840347d9a27d&chksm=8be75b90bc90d286a5af3ef8e610e822d705dc3cf4382b45e3f14489f3e7ec4fd8c95ed0eceb&mpshare=1&scene=2&srcid=0511LMlj9Qv9DeIZAjMjYAU9&sharer_sharetime=1620731348139&sharer_shareid=631c113940cb81f34895aa25ab14422a#rd) and AI Chinese New Star Outstanding Scholar (10 candidates worldwide each year). 

My research interest includes speech synthesis, neural machine translation and automatic music generation. 

I have published 50+ papers <a href='https://scholar.google.com/citations?user=4FA6C0AAAAAJ'><img src="https://img.shields.io/endpoint?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FRayeRen%2Frayeren.github.io@google-scholar-stats%2Fgs_data_shieldsio.json&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a> at the top international AI conferences such as NeurIPS, ICML, ICLR, KDD. 


# üî• News
- *2022.02*: &nbsp;üéâüéâ AAAAAAAAAAAAAAAAAAAAA. 
- *2022.02*: &nbsp;üéâüéâ CCCCCCCCCCCCCCCCCCCCCCC. 


# üìù Publications 
##  üìö TOPIC 1


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2019</div><img src='images/hh.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[ Stacked Intelligent Metasurfaces for Efficient Holographic MIMO Communications in 6G](https://arxiv.org/pdf/2305.08079) <br>
**Yi Ren**, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu

- FastSpeech is the first fully parallel end-to-end speech synthesis model.
- **Academic Impact**: This work is included by many famous speech synthesis open-source projects, such as.....
- **Industry Impact**: FastSpeech has been deployed in ..., and supports 49 more languages with state-of-the-art AI quality. It was also shown as a text-to-speech system acceleration example in ....
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2021</div><img src='images/hh.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA](https://arxiv.org/pdf/2305.08079) <br>
**Yi Ren**, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu

  - This work is included by many famous speech synthesis open-source projects, such as .....
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2024</div><img src='images/hh.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB](https://arxiv.org/pdf/2305.08079) 
Ziyue Jiang, Jinglin Liu, **Yi Ren**, et al.

  - This work has been deployed on many TikTok products.
  - Advandced zero-shot voice cloning model.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2022</div><img src='images/hh.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC](https://arxiv.org/pdf/2305.08079) 
Jinglin Liu, Chengxi Li, **Yi Ren**, Feiyang Chen, Zhou Zhao

- Many [video demos](https://www.bilibili.com/video/BV1be411N7JA) created by the [DiffSinger community](https://github.com/openvpi) are released.
- DiffSinger was introduced in [a very popular video](https://www.bilibili.com/video/BV1uM411t7ZJ) (1600k+ views) on Bilibili!

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2021</div><img src='images/hh.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD](https://arxiv.org/pdf/2305.08079) 
**Yi Ren**, Jinglin Liu, Zhou Zhao

</div>
</div>

- `AAAI 2024` [EEEEEEEEEEEEEEEEEEEEEEEEEE](https://arxiv.org/pdf/2305.08079), Rui Liu, Yifan Hu, **Yi Ren**, et al. 
  
- ``ICML 2023`` [FFFFFFFFFFFFFFFFFFFFFFFFFFFFFF](https://text-to-audio.github.io/paper.pdf), Rongjie Huang, Jiawei Huang, Dongchao Yang, **Yi Ren**, et al.
  

## üìö TOPIC 2

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2024</div><img src='images/hh.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[EEEEEEEEEEEEEEEEEEEEEEEEEE](https://arxiv.org/pdf/2305.08079), Zhenhui Ye, Tianyun Zhong, Yi Ren, et al. <span style="color:red">(Spotlight)</span>  [**Code**](https://github.com/JianchengAn/SIM-1-MIMO)
</div>
</div>


- `AAAI 2024` [EEEEEEEEEEEEEEEEEEEEEEEEEE](https://arxiv.org/pdf/2305.08079), Rui Liu, Yifan Hu, **Yi Ren**, et al. 

## üìö TOPIC 3

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2024</div><img src='images/hh.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB](https://arxiv.org/pdf/2305.08079) 
Ziyue Jiang, Jinglin Liu, **Yi Ren**, et al.

  - This work has been deployed on many TikTok products.
  - Advandced zero-shot voice cloning model.
</div>
</div>

 
- ``ACL 2023`` [AV-TranSpeech: Audio-Visual Robust Speech-to-Speech Translation](), Rongjie Huang, Huadai Liu, Xize Cheng, **Yi Ren**, et al.
- `AAAI 2024` [EEEEEEEEEEEEEEEEEEEEEEEEEE](https://arxiv.org/pdf/2305.08079), Rui Liu, Yifan Hu, **Yi Ren**, et al. 


## Others
- `NeurIPS 2023` [Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective](https://openreview.net/forum?id=Rp4PA0ez0m), Pengfei Wei, Lingdong Kong, Xinghua Qu, **Yi Ren**, et al.
- ``ACM-MM 2022`` [Video-Guided Curriculum Learning for Spoken Video Grounding](), Yan Xia, Zhou Zhao, Shangwei Ye, Yang Zhao, Haoyuan Li, **Yi Ren**

# üìñ Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üéñ Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
